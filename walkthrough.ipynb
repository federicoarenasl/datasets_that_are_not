{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685e9bac-38d1-4648-b76d-1cdc06ff489c",
   "metadata": {},
   "source": [
    "## Winner Take All Convolutional Autoencoders\n",
    "Let's walk through training and investigating a winner take all convolutional autoencoder on the MNIST dataset. We'll start by importing the necessary libraries and loading the dataset. We'll visualize training in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da5164-41a9-47fe-97e9-aa159a4d8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tools.dataset import split_dataset\n",
    "import random\n",
    "from tools.eval import load_model_from_checkpoint, visualize_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227b12d",
   "metadata": {},
   "source": [
    "Define constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.05\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8e9d5-54d7-4db6-be2f-6deab3e84728",
   "metadata": {},
   "source": [
    "Make sure the current device is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133fd72-47e6-48f4-8962-a235ae53a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f870e-e9b0-4186-bc66-90bc43a09989",
   "metadata": {},
   "source": [
    "### Import and prepare MNIST dataset\n",
    "We will work with the MNIST dataset for experimenation and setup. Let's download it using the handy `torchvision.MNIST` datasets. We will first prepare our train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96caac9e-c43e-4587-8d70-37f20d385f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "dataset = datasets.MNIST(\n",
    "    root=\"~/.pytorch/MNIST_data/\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"~/.pytorch/MNIST_data/\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682d0e0-3b86-406d-8842-c2e6fde0d168",
   "metadata": {},
   "source": [
    "### Prepare dataloaders\n",
    "The dataloaders will be helpful to let us access the dataset in batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b71fc-5f97-41f4-b544-ef9014e200f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "train_loader, validation_loader = split_dataset(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b0092",
   "metadata": {},
   "source": [
    "Import implemented models and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b59763-893c-4040-96f4-002abc6f7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tools.train import train_for_n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01aa1fa",
   "metadata": {},
   "source": [
    "## Train autoencoders\n",
    "Let's train the autoencoder and visualize the training in TensorBoard. We will train the autoencoder for 10 epochs. The goal of this excercise is to make sure the autoencoder is learning something useful. We will not be using the autoencoder for any downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5ec21-82b3-41af-837a-c2fbb3c86f92",
   "metadata": {},
   "source": [
    "Import the model from our models repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9019bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.wta import (\n",
    "    WTAConvAutoencoder128,\n",
    "    WTAConvAutoencoder64,\n",
    "    ConvAutoencoder128,\n",
    "    ConvAutoencoder64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fe59d",
   "metadata": {},
   "source": [
    "Let's define the optimizer and loss criterion we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c638613",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894091d4",
   "metadata": {},
   "source": [
    "Train baseline Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "N_EPOCHS = 100\n",
    "VISUALIZE_EVERY = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "CHECKPOINT_PATH = \"/home/fede/Documents/datasets_that_are_not/checkpoints\"\n",
    "\n",
    "# Initialize model\n",
    "model = ConvAutoencoder64()\n",
    "model.to(device)\n",
    "print(f\"Model architecture:\\n\\n{model}\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup summary writer and set training going\n",
    "writer = SummaryWriter(\n",
    "    f\"logs/{model.name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "train_for_n_epochs(\n",
    "    N_EPOCHS,\n",
    "    VISUALIZE_EVERY,\n",
    "    model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    writer=writer,\n",
    "    checkpoint_dir=CHECKPOINT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CHECKPOINTS = \\\n",
    "    \"/home/fede/Documents/datasets_that_are_not/checkpoints/ConvAutoencoder64_20231001-153026/epoch_50.pth\"\n",
    "model = load_model_from_checkpoint(\n",
    "    ConvAutoencoder64(), \n",
    "    PATH_TO_CHECKPOINTS, \n",
    "    device, \n",
    "    eval=True\n",
    ")\n",
    "visualize_filters(model, model.decoder[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb677e3-285d-4f17-b924-6321452d7001",
   "metadata": {},
   "source": [
    "## Train Winner Takes All Convolutional Autoencoder with Spatial Sparsity and Lifetime Sparsity\n",
    "Let's train our Autoencoder architecture with a basic training loop to verify outputs and such. We will train \n",
    "the model that uses architecture `128conv5-128conv5-128deconv1` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50262cfe-752a-4c35-9015-101cf27a9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "VISUALIZE_EVERY = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "CHECKPOINT_PATH = \"/home/fede/Documents/datasets_that_are_not/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c98bf-ef6c-4f59-a21a-e58c8940efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "K_PERCENTAGE = 0.2\n",
    "model = WTAConvAutoencoder128(\n",
    "    k_percentage=K_PERCENTAGE,\n",
    ")\n",
    "model.name = f\"{model.name}_k_{K_PERCENTAGE}\"\n",
    "model.to(device)\n",
    "print(f\"Model architecture:\\n\\n{model}\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089f006-0cba-4045-8374-4a909c2afba7",
   "metadata": {},
   "source": [
    "Train model for `N_EPOCHS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075548f-70dc-44fd-b496-e1ed90c8a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup summary writer and set training going\n",
    "writer = SummaryWriter(\n",
    "    f\"logs/{model.name}_{K_PERCENTAGE}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "train_for_n_epochs(\n",
    "    N_EPOCHS,\n",
    "    VISUALIZE_EVERY,\n",
    "    model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    writer=writer,\n",
    "    checkpoint_dir=CHECKPOINT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f0b50",
   "metadata": {},
   "source": [
    "Train model that uses architecture `64conv5-64conv5-64conv5-64deconv11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d24aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "K_PERCENTAGE = 0.2\n",
    "model = WTAConvAutoencoder64(\n",
    "    k_percentage=1\n",
    ")\n",
    "model.name = f\"{model.name}_k_{1}\"\n",
    "model.to(device)\n",
    "print(f\"Model architecture:\\n\\n{model}\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"Model name is {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c854a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup summary writer and set training going\n",
    "N_EPOCHS = 100\n",
    "writer = SummaryWriter(\n",
    "    f\"logs/{model.name}_{K_PERCENTAGE}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "train_for_n_epochs(\n",
    "    N_EPOCHS,\n",
    "    VISUALIZE_EVERY,\n",
    "    model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    writer=writer,\n",
    "    checkpoint_dir=CHECKPOINT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e89aa1",
   "metadata": {},
   "source": [
    "## Visualizing learned filters from WTA Convolutional Autoencoder\n",
    "Let's visualize the learned filters from the convolutional layers of the autoencoder. We will visualize the filters from the first convolutional layer of the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9138a9",
   "metadata": {},
   "source": [
    "Visualizing the learned deconvolution filters when keeping the top 5% of the activations trained for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713367ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CHECKPOINTS = \\\n",
    "    \"/home/fede/Documents/datasets_that_are_not/checkpoints/WTAConvAutoencoder64_k_0.05_20231001-141318/epoch_69.pth\"\n",
    "model = load_model_from_checkpoint(\n",
    "    WTAConvAutoencoder64(), \n",
    "    PATH_TO_CHECKPOINTS, \n",
    "    device, \n",
    "    eval=True\n",
    ")\n",
    "visualize_filters(model, model.decoder[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce4c3b",
   "metadata": {},
   "source": [
    "Visualizing the learned deconvolution filters when keeping the top 20% of the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CHECKPOINTS = \\\n",
    "    \"/home/fede/Documents/datasets_that_are_not/checkpoints/WTASpatialLifetimeSparseConvAutoencoder_20231001-124328/epoch_29.pth\"\n",
    "model = load_model_from_checkpoint(\n",
    "    WTAConvAutoencoder128(), \n",
    "    PATH_TO_CHECKPOINTS, \n",
    "    device, \n",
    "    eval=True\n",
    ")\n",
    "visualize_filters(model, model.decoder[0].weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
