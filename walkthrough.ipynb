{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685e9bac-38d1-4648-b76d-1cdc06ff489c",
   "metadata": {},
   "source": [
    "## Winner Take All Convolutional Autoencoders\n",
    "Let's walk through training and investigating a winner take all convolutional autoencoder on the MNIST dataset. We'll start by importing the necessary libraries and loading the dataset. We'll visualize training in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22da5164-41a9-47fe-97e9-aa159a4d8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tools.dataset import split_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227b12d",
   "metadata": {},
   "source": [
    "Define constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c6327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8e9d5-54d7-4db6-be2f-6deab3e84728",
   "metadata": {},
   "source": [
    "Make sure the current device is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f133fd72-47e6-48f4-8962-a235ae53a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch running on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f870e-e9b0-4186-bc66-90bc43a09989",
   "metadata": {},
   "source": [
    "### Import and prepare MNIST dataset\n",
    "We will work with the MNIST dataset for experimenation and setup. Let's download it using the handy `torchvision.MNIST` datasets. We will first prepare our train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96caac9e-c43e-4587-8d70-37f20d385f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "dataset = datasets.MNIST(\n",
    "    root=\"~/.pytorch/MNIST_data/\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"~/.pytorch/MNIST_data/\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682d0e0-3b86-406d-8842-c2e6fde0d168",
   "metadata": {},
   "source": [
    "### Prepare dataloaders\n",
    "The dataloaders will be helpful to let us access the dataset in batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "572b71fc-5f97-41f4-b544-ef9014e200f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "train_loader, validation_loader = split_dataset(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93591448-7bfc-444a-8801-862eb3495153",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc007e80-5311-4c04-9a1a-083e3fc731db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd4db49cf70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (2,2)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b0092",
   "metadata": {},
   "source": [
    "Import implemented models and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b59763-893c-4040-96f4-002abc6f7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tools.train import train_for_n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01aa1fa",
   "metadata": {},
   "source": [
    "## Train autoencoders\n",
    "Let's train the autoencoder and visualize the training in TensorBoard. We will train the autoencoder for 10 epochs. The goal of this excercise is to make sure the autoencoder is learning something useful. We will not be using the autoencoder for any downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5ec21-82b3-41af-837a-c2fbb3c86f92",
   "metadata": {},
   "source": [
    "Import the model from our models repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9019bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.convautoencoders import (\n",
    "    ConvAutoencoder,\n",
    "    WTASpatialConvAutoencoder,\n",
    "    WTALifetimeSparseConvAutoencoder,\n",
    "    WTASpatialLifetimeSparseConvAutoencoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3746937",
   "metadata": {},
   "source": [
    "\n",
    "Let's define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c873dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "VISUALIZE_EVERY = 1\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fe59d",
   "metadata": {},
   "source": [
    "Let's define the optimizer and loss criterion we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c638613",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc2fed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train Winner Takes All Convolutional Autoencoder with Spatial Sparsity\n",
    "Let's train our Autoencoder architecture with a basic training loop to verify outputs and such."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d3ef9",
   "metadata": {},
   "source": [
    "Import the model from our models repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb682a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wta_model = WTASpatialConvAutoencoder()\n",
    "wta_model.to(device)\n",
    "print(f\"Model architecture:\\n\\n{wta_model}\")\n",
    "optimizer = torch.optim.Adam(wta_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace31db",
   "metadata": {},
   "source": [
    "Train model for `N_EPOCHS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\n",
    "    f\"logs/{wta_model.name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "train_for_n_epochs(\n",
    "    N_EPOCHS,\n",
    "    VISUALIZE_EVERY,\n",
    "    wta_model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    checkpoint_dir=\"models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c485b-4b97-475c-97fc-eefe9edc7be6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train Winner Takes All Convolutional Autoencoder with Lifetime Sparsity\n",
    "Let's train our Autoencoder architecture with a basic training loop to verify outputs and such."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a6c83-20b6-482c-965d-440080b5851e",
   "metadata": {},
   "source": [
    "Import the model from our models repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7485e-d299-4287-b3b9-502339421df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wta_lifetime_model = WTALifetimeSparseConvAutoencoder(k_percentage=0.9)\n",
    "wta_lifetime_model.to(device)\n",
    "print(f\"Model architecture:\\n\\n{wta_lifetime_model}\")\n",
    "optimizer = torch.optim.Adam(wta_lifetime_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fbc1a-70b5-472d-9bbb-9b8c355d6e3f",
   "metadata": {},
   "source": [
    "Set up basic training hyper parameters and TensorBoard training visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827ca7e-f752-4415-95fc-930ddd64f68a",
   "metadata": {},
   "source": [
    "Train model for `N_EPOCHS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156911bb-07b9-40e7-845b-55ad33374c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\n",
    "    f\"logs/{wta_lifetime_model.name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "train_for_n_epochs(\n",
    "    N_EPOCHS,\n",
    "    VISUALIZE_EVERY,\n",
    "    wta_lifetime_model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb677e3-285d-4f17-b924-6321452d7001",
   "metadata": {},
   "source": [
    "## Train Winner Takes All Convolutional Autoencoder with Spatial Sparsity and Lifetime Sparsity\n",
    "Let's train our Autoencoder architecture with a basic training loop to verify outputs and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50262cfe-752a-4c35-9015-101cf27a9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_PERCENTAGE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c98bf-ef6c-4f59-a21a-e58c8940efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "wta_spatial_lifetime_model = WTASpatialLifetimeSparseConvAutoencoder(\n",
    "    k_percentage=K_PERCENTAGE\n",
    ")\n",
    "wta_spatial_lifetime_model.to(device)\n",
    "print(f\"Model architecture:\\n\\n{wta_spatial_lifetime_model}\")\n",
    "optimizer = torch.optim.Adam(wta_spatial_lifetime_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089f006-0cba-4045-8374-4a909c2afba7",
   "metadata": {},
   "source": [
    "Train model for `N_EPOCHS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075548f-70dc-44fd-b496-e1ed90c8a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup summary writer and set training going\n",
    "writer = SummaryWriter(\n",
    "    f\"logs/{wta_spatial_lifetime_model.name}_{K_PERCENTAGE}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "train_for_n_epochs(\n",
    "    N_EPOCHS,\n",
    "    VISUALIZE_EVERY,\n",
    "    wta_spatial_lifetime_model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918eb89-5a76-4b71-b410-d37ab0d864aa",
   "metadata": {},
   "source": [
    "Experiment with multiple percentages from `0.1` to `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611de18-1c9f-48f9-a5b8-706580504f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_PERCENTAGES = np.linspace(0.1, 1, 10)\n",
    "for K_PERCENTAGE in K_PERCENTAGES:\n",
    "    print(f\"Training WTA Autoencoder with lifetime sparsity k% {K_PERCENTAGE}.\")\n",
    "    # Initialize model\n",
    "    wta_spatial_lifetime_model = WTASpatialLifetimeSparseConvAutoencoder(\n",
    "        k_percentage=K_PERCENTAGE\n",
    "    )\n",
    "    wta_spatial_lifetime_model.to(device)\n",
    "    # Setup summary writer and set training going\n",
    "    writer = SummaryWriter(\n",
    "        f\"logs/{wta_spatial_lifetime_model.name}_{K_PERCENTAGE}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "    train_for_n_epochs(\n",
    "        N_EPOCHS,\n",
    "        VISUALIZE_EVERY,\n",
    "        wta_spatial_lifetime_model,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        writer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e89aa1",
   "metadata": {},
   "source": [
    "## Visualizing learned filters from WTA Convolutional Autoencoder\n",
    "Let's visualize the learned filters from the convolutional layers of the autoencoder. We will visualize the filters from the first convolutional layer of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CHECKPOINTS = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7983ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wta_conv_autoencoder = WTASpatialLifetimeSparseConvAutoencoder()\n",
    "wta_conv_autoencoder.load_state_dict(torch.load(PATH_TO_CHECKPOINTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462faa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, filter in enumerate(wta_conv_autoencoder.encoder[0].weight):\n",
    "    ax = fig.add_subplot(8, 8, i + 1)\n",
    "    ax.imshow(filter.detach().cpu().squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect(\"equal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
